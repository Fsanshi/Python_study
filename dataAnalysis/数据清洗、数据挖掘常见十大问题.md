# 数据清洗、数据挖掘常见十大问题



## 一、数据预处理、数据清洗和特征工程

> ​	**数据预处理主要是对数据集进行探索性分析，而特征工程则是进行分析后的相应处理**。以上两个名词不常说，最常听的应该还是数据清洗，差不多也就相当于上面两个步骤。



> ​	例如：在预处理阶段发现数据存在缺失值、异常值；数据特征之间存在共线性；数据特征可以互相组合形成更好的特征等等。就可以在特征工程 阶段对上述发现的问题进行相应的处理。



## 二、数据预处理和特征工程阶段**最常见的10个问题**

### 1. 什么是数据 EDA？

EDA：Exploratory Data Analysis，译为：探索性数据分析。

​	数据 EDA 是在拿到数据之后对数据进行初步探索认识的一个过程，在数据 EDA 阶段，并不对数据做任何处理，只进行数据探索，而在特征工程阶段会对数据进行相关操作。具体的，数据 EDA 有如下作用：

1. 了解数据的分布、特征的类别，以及发现离群点数据。这一步可通过简单的直方图、散点图、小提琴图、箱型图等进行探索；
2. 了解数据特征与特征之间的关联情况，以及特征与目标变量之间的关系。这一步可通过组合直方图、热力图等进行探索；
3. 对于划分后的数据集，可以探索训练集和测试集的样本整体分布是否一致，数据特征的缺失情况、分布是否一致等。



### 2. 缺失值的处理方式有哪些？

对于缺失值的处理有很多方法，在缺失率低的情况下可以对缺失数据进行填充，比如使用均值、众数、随机森林算法等进行缺失值填充；

另外，如果缺失值不能简单的填充，可以将缺失数据当做特征中的某个类别处理（具体的也可以在数据 EDA 中探索数据缺失的情况下和目标变量之间的关系）

如果某个特征的缺失程度过高，也可以直接剔除该特征。需要注意的是，在 xgb 和 lgb 模型中可以自动处理缺失值，所以不需要提前进行处理。



### 3. 如何检测异常数据？如何处理？

异常数据的检测有两种方法，**基于统计的异常点检测**和**基于距离的异常点检测**。

基于统计的异常点检测常用的有四分位法，通过上下四分位对异常数据进行筛选，特别的，在数据 EDA 阶段可以通过箱型图、小提琴图进行类似原理的检测。

基于距离的异常点检测可以参考聚类模型，通过欧氏距离公式计算点点之间的距离，并据此筛选异常数据。

对于异常数据，可以替换也可以删除；特别的，在风控模型中，会通过 WOE 转换对数据进行处理，将数据分成一箱一箱的，据此可以消除异常值对整体数据的影响。



### 4. 什么是特征工程？有什么作用？

特征工程总体来说是对数据进行处理、转换、筛选等，对在数据 EDA 阶段发现的缺失数据、异常数据等，都会在特征工程中进行处理，另外，对于特征的衍生、组合、转换等操作也会在此进行。

特征工程的目的就是通过数据预处理、特征衍生、特征筛选从而得到**规整的数据**和**贡献度大的特征**，使模型达到更好的效果。



### 5. 特征工程的一般步骤是什么？

特征工程的一般步骤包括数据预处理，特征转换和特征筛选三部分。

1. 数据预处理：主要对缺失值、异常值、数据格式等进行简单的处理操作；

2. 特征转换：对连续特征、离散特征、时间序列特征等进行转换，更进一步的，还会对特征之间进行特征组合，包括但不限于四则运算、交叉、合并等业务上的特征操作；

   > 例如：未婚 + 本科毕业，男生 + 有房有车

3. 特征筛选：在上一步生成的大量的特征中筛选部分对目标变量有明显贡献的特征，常用的方法有 **过滤法、包装法和嵌入法**（后面会具体介绍）。



### 6. 特征衍生的方法有哪些？

常用的特征衍生主要包括业务上的衍生和非业务上的衍生，整理如下：

- 业务上的特征衍生：基于对业务的深入理解，进行头脑风暴，或者整合第三方的数据进行业务上的交叉和延伸
- 非业务上的特征衍生：抛开业务本身，对于特征可以进行四则运算、取平均/最大/最小、单位转换等操作；另外，对于类别特征，还可以进行独热编码等衍生操作。



### 7. 对于时间序列特征、连续特征、离散特征如何做特征转换的？

1. 对于时间序列特征：将时间变量的维度进行分离（年/月/日/时/分/秒），或者进行简单的衍生（季度、星期、凌晨、中午等），更进一步的可以与其他变量进行组合
2. 对于连续型特征：常用标准化、归一化、离散化等操作。评分卡模型中主要用到离散化分箱，常用的离散化方法有：卡方分箱、等频等距分箱等。
3. 对于离散型特征：如果是无序离散可以用独热编码，如果是有序离散可以用顺序编码。如果类别数较多可以使用平均数编码



### 8. 如何处理样本不平衡问题？

风控模型中样本不平衡主要是因为坏样本的数量太少，坏样本受限于用户本来就较少，也因为风控策略的严格导致坏用户过少。在针对此类数据样本一般使用如下方法：

- **尝试扩大数据集**，比如通过延长时间线来收集数据，将三个月的用户数据延长到六个月以增加数据量；
- **对数据集进行抽样**，一种是进行欠采样，通过减少较多类的数据样本来降低数据的不平衡；另一种是进行过采样，通过增加较少类的数据样本来降低数据的不平衡，常用 SMOTE 方法来实现过采样；
- 尝试使用对不平衡样本数据处理效果较好的模型，如 xgb 和 lgb 模型。



### 9. 特征筛选的作用和目的？

在开始建模前的最后一个步骤就是进行特征筛选，特征筛选就是从所有的特征中筛选出贡献度最高的 m 个特征，使用筛选后的特征建模后有如下好处：

- 大大缩短模型训练的时间，特别是在评分卡模型数据维度特别多时效果更佳；
- 简化模型，避免维度过多产生维度灾难；
- 增加模型的可解释性，减低模型过拟合的风险。



### 10. 特征筛选的方法有哪些？优缺点各是什么？

特征筛选常用的方法有过滤法、封装法和嵌入法，如何如下：

#### 10.1 过滤法 Filter：

按照发散性或者相关性对各个特征进行评分，手动设定阈值或者待选择阈值的个数，选择特征。比较常用的方法有：**方差过滤、卡方齐性检验、互信息法过滤、相关系数过滤、IV 值过滤**，其中，后两个较常用。

- 优点：算法复杂度低、通用性强，不需要训练分类器，对于大规模数据集比较实用；
- 缺点：对于特征的评分在准确率上一般较低。

#### 10.2 嵌入法 Embedded

先使用某些机器学习算法进行模型训练，得到各个特征的权重系数，根据系数从大到小选择特征。比较常用的方法有：**基于随机森林、xgb、lgb 的嵌入法和使用惩罚项的模型的嵌入法（如岭回归，lasso 回归等）**

上述提到的权重系数代表特征对于模型的某种贡献或重要性，比较树模型中的 feature_importances_ 属性。

- 优点：更加精确到模型的效用本身，对于模型性能的提升较好
- 缺点：特征对于模型贡献度的阈值无法主观确定，需要根据实际情况确定。

#### 10.3 包装法 Wrapper：

与嵌入法类似，包装法是一个特征选择和算法训练同时进行的方法，比较依赖于算法自身的选择。比较常用的方法有：**递归消除法、启发式搜索（前向/后向选择法，逐步选择法）、随机搜索**。启发式搜索较常用。

具体的，包装法在初始训练集上训练评估器，通过 coed_ 属性或者通过 feature_importances_ 属性获得每个特征的重要性；然后，从当前的一组特征中修剪最不重要的特征，重复递归该过程直到特征达到要求

- 优点：相对于过滤法，封装法的分类性能会更好
- 缺点：通用性不强，当改变学习算法时，需要针对该学习算法重新进行特征选择；并且对于大规模数据，执行时间较长。

